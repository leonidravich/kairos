{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg_ctl: another server might be running; trying to start server anyway\n",
      "waiting for server to start.... stopped waiting\n",
      "pg_ctl: could not start server\n",
      "Examine the log output.\n"
     ]
    }
   ],
   "source": [
    "!pg_ctl -D /home/ravich/mylocal_db -l logfile start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import gc\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device='cpu'\n",
    "# msg structure:      [src_node_feature,edge_attr,dst_node_feature]\n",
    "\n",
    "# compute the best partition \n",
    "import datetime\n",
    "# import community as community_louvain\n",
    "\n",
    "import xxhash\n",
    "#  Find the edge index which the edge vector is corresponding to\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.cpu().numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()\n",
    "\n",
    "\n",
    "def cal_pos_edges_loss(link_pred_ratio):\n",
    "    loss=[]\n",
    "    for i in link_pred_ratio:\n",
    "        loss.append(criterion(i,torch.ones(1)))\n",
    "    return torch.tensor(loss)\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio,labels):\n",
    "    loss=[] \n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1,-1),labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pos_edges_loss_autoencoder(decoded,msg):\n",
    "    loss=[] \n",
    "    for i in range(len(decoded)):\n",
    "        loss.append(criterion(decoded[i].reshape(-1),msg[i].reshape(-1)))\n",
    "    return torch.tensor(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time as pytime\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = pytime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(pytime.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = pytime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = pytime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id={1: 'EVENT_CONNECT',\n",
    " 'EVENT_CONNECT': 1,\n",
    " 2: 'EVENT_EXECUTE',\n",
    " 'EVENT_EXECUTE': 2,\n",
    " 3: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 3,\n",
    " 4: 'EVENT_READ',\n",
    " 'EVENT_READ': 4,\n",
    " 5: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 5,\n",
    " 6: 'EVENT_RECVMSG',\n",
    " 'EVENT_RECVMSG': 6,\n",
    " 7: 'EVENT_SENDMSG',\n",
    " 'EVENT_SENDMSG': 7,\n",
    " 8: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 8,\n",
    " 9: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from psycopg2 import extras as ex\n",
    "connect = psycopg2.connect(database = 'tc_theia_dataset_db',\n",
    "                           host = 'localhost',\n",
    "                           user = 'ravich',\n",
    "                           password = '',\n",
    "                           port = ''\n",
    "                          )\n",
    "\n",
    "cur = connect.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the map for nodeid to msg\n",
    "sql=\"select * from node2id ORDER BY index_id;\"\n",
    "cur.execute(sql)\n",
    "rows = cur.fetchall()\n",
    "\n",
    "nodeid2msg={}  # nodeid => msg and node hash => nodeid\n",
    "for i in rows:\n",
    "    nodeid2msg[i[0]]=i[-1]\n",
    "    nodeid2msg[i[-1]]={i[1]:i[2]}  #0-828297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_3=torch.load(\"./train_graph/graph_4_3.TemporalData.simple\").to(device=device)\n",
    "graph_4_4=torch.load(\"./train_graph/graph_4_4.TemporalData.simple\").to(device=device)\n",
    "graph_4_5=torch.load(\"./train_graph/graph_4_5.TemporalData.simple\").to(device=device)\n",
    "graph_4_9=torch.load(\"./train_graph/graph_4_9.TemporalData.simple\").to(device=device)\n",
    "\n",
    "graph_4_10=torch.load(\"./train_graph/graph_4_10.TemporalData.simple\").to(device=device)\n",
    "graph_4_11=torch.load(\"./train_graph/graph_4_11.TemporalData.simple\").to(device=device)\n",
    "graph_4_12=torch.load(\"./train_graph/graph_4_12.TemporalData.simple\").to(device=device)\n",
    "graph_4_13=torch.load(\"./train_graph/graph_4_13.TemporalData.simple\").to(device=device)\n",
    "train_data=graph_4_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[696391, 711416, 713277, 713482, 767073, 798797, 828298, 828212]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    graph_4_3.num_nodes,\n",
    "    graph_4_4.num_nodes,\n",
    "    graph_4_5.num_nodes,\n",
    "    graph_4_9.num_nodes,\n",
    "    graph_4_10.num_nodes,\n",
    "    graph_4_11.num_nodes,\n",
    "    graph_4_12.num_nodes,\n",
    "    graph_4_13.num_nodes,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_node_num = 828398 \n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=20, device=device)\n",
    "\n",
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(in_channels*8, out_channels,heads=1, concat=False,\n",
    "                             dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels*2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels*2)\n",
    "        \n",
    "        self.lin_seq = nn.Sequential(\n",
    "            \n",
    "            Linear(in_channels*4, in_channels*8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*8, in_channels*2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels*2, int(in_channels//2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(int(in_channels//2), train_data.msg.shape[1]-32)                   \n",
    "        )\n",
    "        \n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([self.lin_src(z_src) , self.lin_dst(z_dst)],dim=-1)      \n",
    "         \n",
    "        h = self.lin_seq (h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "memory_dim = time_dim = embedding_dim = 100\n",
    "\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.00005, eps=1e-08,weight_decay=0.01)\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "saved_nodes=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1024\n",
    "def train(train_data):\n",
    "\n",
    "    \n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    saved_nodes=set()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_batchs=len(train_data.t)//BATCH +1\n",
    "    batch_index=0\n",
    "#     print(\"train_before_stage_data:\",train_data)\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        start = time.perf_counter()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg        \n",
    "        \n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "#         n_id = torch.cat([src, pos_dst, neg_src, neg_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "        \n",
    "#         print(f\"{n_id=}\")\n",
    "#         print(f\"{edge_index=}\")\n",
    "#         print(f\"{train_data.msg[e_id]=}\")\n",
    "      \n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "        \n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])       \n",
    "\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        \n",
    "#         y_true = torch.cat([torch.zeros(pos_out.size(0),1),torch.ones(neg_out.size(0),1)], dim=0)\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l)           \n",
    "          \n",
    "        y_true = torch.tensor(y_true)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "        \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "\n",
    "#         loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "#         loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "#         for i in range(len(src)):\n",
    "#             saved_nodes.add(int(src[i]))\n",
    "#             saved_nodes.add(int(pos_dst[i]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "#         print(z.shape)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "        batch_index+=1\n",
    "        end = time.perf_counter()\n",
    "#         print(f\"current epoch process:{(batch_index/total_batchs):.4f}%   cost time:{(end-start):.2f}\")\n",
    "#     print(\"trained_stage_data:\",train_data)\n",
    "    return total_loss / train_data.num_events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 0.6211\n",
      "  Epoch: 01, Loss: 0.3078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [20:26<16:41:57, 1226.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 0.3502\n",
      "  Epoch: 02, Loss: 0.2702\n",
      "  Epoch: 02, Loss: 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [40:18<16:04:54, 1206.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 02, Loss: 0.3259\n",
      "  Epoch: 03, Loss: 0.2435\n",
      "  Epoch: 03, Loss: 0.2735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [59:56<15:34:35, 1193.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 03, Loss: 0.3177\n",
      "  Epoch: 04, Loss: 0.2654\n",
      "  Epoch: 04, Loss: 0.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [1:19:32<15:09:31, 1186.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 04, Loss: 0.3082\n",
      "  Epoch: 05, Loss: 0.2711\n",
      "  Epoch: 05, Loss: 0.2712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [1:39:07<14:46:42, 1182.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 05, Loss: 0.3103\n",
      "  Epoch: 06, Loss: 0.2659\n",
      "  Epoch: 06, Loss: 0.2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [1:58:24<14:20:43, 1173.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 06, Loss: 0.3036\n",
      "  Epoch: 07, Loss: 0.2562\n",
      "  Epoch: 07, Loss: 0.2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [2:17:46<13:58:23, 1169.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 07, Loss: 0.3061\n",
      "  Epoch: 08, Loss: 0.2629\n",
      "  Epoch: 08, Loss: 0.2673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [2:37:32<13:42:28, 1174.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 08, Loss: 0.3043\n",
      "  Epoch: 09, Loss: 0.2533\n",
      "  Epoch: 09, Loss: 0.2689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [2:57:00<13:21:27, 1172.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 09, Loss: 0.3068\n",
      "  Epoch: 10, Loss: 0.2637\n",
      "  Epoch: 10, Loss: 0.2633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [3:15:45<12:52:04, 1158.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 10, Loss: 0.3056\n",
      "  Epoch: 11, Loss: 0.2605\n",
      "  Epoch: 11, Loss: 0.2623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [3:34:31<12:26:30, 1148.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 11, Loss: 0.3060\n",
      "  Epoch: 12, Loss: 0.2591\n",
      "  Epoch: 12, Loss: 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [3:53:15<12:02:38, 1141.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 12, Loss: 0.3075\n",
      "  Epoch: 13, Loss: 0.2578\n",
      "  Epoch: 13, Loss: 0.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [4:12:01<11:40:44, 1136.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 13, Loss: 0.2995\n",
      "  Epoch: 14, Loss: 0.2563\n",
      "  Epoch: 14, Loss: 0.2617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [4:30:54<11:21:12, 1135.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 14, Loss: 0.2960\n",
      "  Epoch: 15, Loss: 0.2559\n",
      "  Epoch: 15, Loss: 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [4:49:45<11:01:32, 1134.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 15, Loss: 0.2999\n",
      "  Epoch: 16, Loss: 0.2555\n",
      "  Epoch: 16, Loss: 0.2591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [5:08:44<10:43:21, 1135.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 16, Loss: 0.3014\n",
      "  Epoch: 17, Loss: 0.2546\n",
      "  Epoch: 17, Loss: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [5:27:49<10:26:04, 1138.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 17, Loss: 0.2981\n",
      "  Epoch: 18, Loss: 0.2555\n",
      "  Epoch: 18, Loss: 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [5:47:10<10:10:51, 1145.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 18, Loss: 0.2857\n",
      "  Epoch: 19, Loss: 0.2543\n",
      "  Epoch: 19, Loss: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [6:06:47<9:56:39, 1154.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 19, Loss: 0.2983\n",
      "  Epoch: 20, Loss: 0.2551\n",
      "  Epoch: 20, Loss: 0.2621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [6:25:34<9:33:14, 1146.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 20, Loss: 0.2880\n",
      "  Epoch: 21, Loss: 0.2531\n",
      "  Epoch: 21, Loss: 0.2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [6:44:10<9:09:34, 1137.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 21, Loss: 0.2915\n",
      "  Epoch: 22, Loss: 0.2546\n",
      "  Epoch: 22, Loss: 0.2593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [7:02:46<8:47:41, 1130.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 22, Loss: 0.2972\n",
      "  Epoch: 23, Loss: 0.2543\n",
      "  Epoch: 23, Loss: 0.2551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [7:21:23<8:26:59, 1126.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 23, Loss: 0.2967\n",
      "  Epoch: 24, Loss: 0.2536\n",
      "  Epoch: 24, Loss: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [7:40:00<8:06:59, 1123.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 24, Loss: 0.2954\n",
      "  Epoch: 25, Loss: 0.2549\n",
      "  Epoch: 25, Loss: 0.2561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [7:58:37<7:47:23, 1121.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 25, Loss: 0.2884\n",
      "  Epoch: 26, Loss: 0.2540\n",
      "  Epoch: 26, Loss: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [8:17:12<7:27:57, 1119.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 26, Loss: 0.2929\n",
      "  Epoch: 27, Loss: 0.2527\n",
      "  Epoch: 27, Loss: 0.2574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [8:35:48<7:08:46, 1118.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 27, Loss: 0.2902\n",
      "  Epoch: 28, Loss: 0.2522\n",
      "  Epoch: 28, Loss: 0.2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [8:54:22<6:49:38, 1117.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 28, Loss: 0.2912\n",
      "  Epoch: 29, Loss: 0.2535\n",
      "  Epoch: 29, Loss: 0.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [9:12:58<6:30:53, 1116.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 29, Loss: 0.2888\n",
      "  Epoch: 30, Loss: 0.2530\n",
      "  Epoch: 30, Loss: 0.2526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [9:31:34<6:12:11, 1116.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 30, Loss: 0.3028\n",
      "  Epoch: 31, Loss: 0.2523\n",
      "  Epoch: 31, Loss: 0.2608\n",
      "  Epoch: 32, Loss: 0.2515\n",
      "  Epoch: 32, Loss: 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [10:08:45<5:34:45, 1115.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 32, Loss: 0.2875\n"
     ]
    }
   ],
   "source": [
    "train_graphs=[\n",
    "    graph_4_3,\n",
    "    graph_4_4, \n",
    "    graph_4_5,\n",
    "#     graph_4_9\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "cpu_start_time = time.process_time()\n",
    "if torch.cuda.is_available():\n",
    "     gpu_start = torch.cuda.Event(enable_timing=True)\n",
    "     gpu_end = torch.cuda.Event(enable_timing=True)\n",
    "     gpu_start.record()\n",
    "\n",
    "for epoch in tqdm(range(1, 51)):\n",
    "    for g in train_graphs:\n",
    "        loss = train(g)\n",
    "        print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "#     scheduler.step()\n",
    "\n",
    "end_time = time.time()\n",
    "cpu_end_time = time.process_time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_end.record()\n",
    "    torch.cuda.synchronize()  # Wait for the events to be recorded\n",
    "    gpu_time = gpu_start.elapsed_time(gpu_end) / 1000  # Convert milliseconds to seconds\n",
    "else:\n",
    "    gpu_time = None\n",
    "\n",
    "total_time = end_time - start_time\n",
    "cpu_time = cpu_end_time - cpu_start_time\n",
    "\n",
    "print(f\"Total Training Time: {total_time:.2f} seconds (Real Time)\")\n",
    "print(f\"CPU Time Used: {cpu_time:.2f} seconds\")\n",
    "if gpu_time is not None:\n",
    "    print(f\"GPU Time Used: {gpu_time:.2f} seconds\")\n",
    "\n",
    "model=[memory,gnn, link_pred,neighbor_loader]\n",
    "os.system(\"mkdir -p ./models/\")\n",
    "torch.save(model,\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass_without_neg_edge.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "g_total_edges = 0\n",
    "g_total_time = 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_day_new(inference_data,path):\n",
    "    global g_total_edges\n",
    "    global g_total_time\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    \n",
    "#     m=torch.load(\"model_saved_emb100.pt\")\n",
    "#     memory,gnn, link_pred,neighbor_loader=m\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "    \n",
    "    memory.reset_state()  # Start with a fresh memory.  \n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    \n",
    "    time_with_loss={}\n",
    "    total_loss = 0    \n",
    "    edge_list=[]\n",
    "    \n",
    "    unique_nodes=torch.tensor([]).to(device=device)\n",
    "    total_edges=0\n",
    "    \n",
    "#     test_memory=copy.deepcopy(memory)   \n",
    "#     test_gnn=copy.deepcopy(gnn)   \n",
    "#     test_link_pred=copy.deepcopy(link_pred) \n",
    "#     test_neighbor_loader=copy.deepcopy(neighbor_loader)\n",
    "\n",
    "\n",
    "\n",
    "    start_time=inference_data.t[0]\n",
    "    event_count=0\n",
    "    \n",
    "    pos_o=[]\n",
    "    \n",
    "    loss_list=[]\n",
    "    \n",
    "#     print(\"before merge:\",train_data)\n",
    "\n",
    "#     nique_node_count=len(torch.cat([train_data.src,train_data.dst]).unique())\n",
    "\n",
    "    print(\"after merge:\",inference_data)\n",
    "    \n",
    "    # Record the running time to evaluate the performance\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        \n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes=torch.cat([unique_nodes,src,pos_dst]).unique()\n",
    "        total_edges+=BATCH\n",
    "        g_total_edges+=BATCH\n",
    "       \n",
    "        n_id = torch.cat([src, pos_dst]).unique()       \n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        \n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "#         y_true = torch.cat(\n",
    "#             [torch.ones(pos_out.size(0))], dim=0).to(torch.long)     \n",
    "#         y_true=y_true.reshape(-1).to(torch.long)\n",
    "\n",
    "        y_true=[]\n",
    "        for m in msg:\n",
    "            l=tensor_find(m[16:-16],1)-1\n",
    "            y_true.append(l) \n",
    "        y_true = torch.tensor(y_true)\n",
    "        y_true=y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "\n",
    "        # Only consider which edge hasn't been correctly predicted.\n",
    "        # For benign graphs, the behaviors patterns are similar and therefore their losses are small\n",
    "        # For anoamlous behaviors, some behaviors might not be seen before, so the probability of predicting those edges are low. Thus their losses are high.\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "     \n",
    "        \n",
    "        # update the edges in the batch to the memory and neighbor_loader\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "        \n",
    "        # compute the loss for each edge\n",
    "        each_edge_loss= cal_pos_edges_loss_multiclass(pos_out,y_true)\n",
    "        \n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode=int(src[i])\n",
    "            dstnode=int(pos_dst[i])  \n",
    "            \n",
    "            srcmsg=str(nodeid2msg[srcnode]) \n",
    "            dstmsg=str(nodeid2msg[dstnode])\n",
    "            t_var=int(t[i])\n",
    "            edgeindex=tensor_find(msg[i][16:-16],1)    \n",
    "            edge_type=rel2id[edgeindex]\n",
    "            loss=each_edge_loss[i]    \n",
    "\n",
    "            temp_dic={}\n",
    "            temp_dic['loss']=float(loss)\n",
    "            temp_dic['srcnode']=srcnode\n",
    "            temp_dic['dstnode']=dstnode\n",
    "            temp_dic['srcmsg']=srcmsg\n",
    "            temp_dic['dstmsg']=dstmsg\n",
    "            temp_dic['edge_type']=edge_type\n",
    "            temp_dic['time']=t_var\n",
    "            \n",
    "\n",
    "#             if \"netflow\" in srcmsg or \"netflow\" in dstmsg:\n",
    "#                 temp_dic['loss']=0\n",
    "            edge_list.append(temp_dic)\n",
    "        \n",
    "        event_count+=len(batch.src)\n",
    "        if t[-1]>start_time+60000000000*15:\n",
    "            # Here is a checkpoint, which records all edge losses in the current time window\n",
    "#             loss=total_loss/event_count\n",
    "            time_interval=ns_time_to_datetime_US(start_time)+\"~\"+ns_time_to_datetime_US(t[-1])\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval]={'loss':loss,\n",
    "                                \n",
    "                                          'nodes_count':len(unique_nodes),\n",
    "                                          'total_edges':total_edges,\n",
    "                                          'costed_time':(end-start)}\n",
    "            \n",
    "            \n",
    "            log=open(path+\"/\"+time_interval+\".txt\",'w')\n",
    "            \n",
    "            for e in edge_list: \n",
    "#                 temp_key=e['srcmsg']+e['dstmsg']+e['edge_type']\n",
    "#                 if temp_key in train_edge_set:      \n",
    "# #                     e['loss']=(e['loss']-train_edge_set[temp_key]) if e['loss']>=train_edge_set[temp_key] else 0  \n",
    "# #                     e['loss']=abs(e['loss']-train_edge_set[temp_key])\n",
    "                    \n",
    "#                     e['modified']=True\n",
    "#                 else:\n",
    "#                     e['modified']=False\n",
    "                loss+=e['loss']\n",
    "\n",
    "            loss=loss/event_count   \n",
    "            print(f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Cost Time: {(end-start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x:x['loss'],reverse=True)   # Rank the results based on edge loss\n",
    "            for e in edge_list: \n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\") \n",
    "            event_count=0\n",
    "            total_loss=0\n",
    "            loss=0\n",
    "            start_time=t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "                   \n",
    "    g_total_time+=time.perf_counter()-start\n",
    "    return time_with_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4-9 ~ 4-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_5.to(device=\"cpu\")\n",
    "graph_4_9.to(device=\"cpu\")\n",
    "graph_4_10.to(device=\"cpu\")\n",
    "graph_4_11.to(device=\"cpu\")\n",
    "graph_4_12.to(device=\"cpu\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path=\"./models/model_saved_emb100_BATCH_1024_LastAggregator_multiclass_without_neg_edge.pt\"\n",
    "#pretrained model\n",
    "model_path=\"/home/ravich/kairos/theia3_models.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_3=test_day_new(graph_4_3,\"graph_4_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_4=test_day_new(graph_4_4,\"graph_4_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_3.to(device=\"cpu\")\n",
    "graph_4_4.to(device=\"cpu\")\n",
    "graph_4_5.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_5=test_day_new(graph_4_5,\"graph_4_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_5.to(device=\"cpu\")\n",
    "graph_4_9.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_9=test_day_new(graph_4_9,\"graph_4_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_9.to(device=\"cpu\")\n",
    "graph_4_10.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_10=test_day_new(graph_4_10,\"graph_4_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_10.to(device=\"cpu\")\n",
    "graph_4_11.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_11=test_day_new(graph_4_11,\"graph_4_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_11.to(device=\"cpu\")\n",
    "graph_4_12.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(model_path,map_location=device)\n",
    "memory,gnn, link_pred,neighbor_loader=model\n",
    "ans_4_12=test_day_new(graph_4_12,\"graph_4_12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_4_12.to(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total inference time: {g_total_time} Total inference edges: {g_total_edges} avg {g_total_edges/g_total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute anomlous score and Initialize the node IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_train_IDF(find_str,file_list):\n",
    "    include_count=0\n",
    "    for f_path in (file_list):\n",
    "        f=open(f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1             \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def cal_IDF(find_str,file_path,file_list):\n",
    "    file_list=os.listdir(file_path)\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1\n",
    "#         add=True\n",
    "#         for line in f:\n",
    "            \n",
    "#             if find_str in line:\n",
    "# #                 print(line)\n",
    "#                 if add:\n",
    "#                     include_count+=1\n",
    "#                     add=False\n",
    "#                 l=line.strip()\n",
    "#                 jdata=eval(l)\n",
    "#                 different_neighbor.add(jdata['srcmsg'])\n",
    "#                 different_neighbor.add(jdata['dstmsg'])\n",
    "                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_IDF_by_file_in_mem(find_str,file_list):\n",
    "\n",
    "    include_count=0\n",
    "    different_neighbor=set()\n",
    "    for f_path in (file_list):\n",
    "        f=open(file_path+f_path)\n",
    "        if find_str in f.read():\n",
    "            include_count+=1\n",
    "#         add=True\n",
    "#         for line in f:\n",
    "            \n",
    "#             if find_str in line:\n",
    "# #                 print(line)\n",
    "#                 if add:\n",
    "#                     include_count+=1\n",
    "#                     add=False\n",
    "#                 l=line.strip()\n",
    "#                 jdata=eval(l)\n",
    "#                 different_neighbor.add(jdata['srcmsg'])\n",
    "#                 different_neighbor.add(jdata['dstmsg'])\n",
    "                \n",
    "                \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    \n",
    "    return IDF,1\n",
    "\n",
    "def cal_redundant(find_str,edge_list):\n",
    "    \n",
    "    different_neighbor=set()\n",
    "    for e in edge_list:\n",
    "        if find_str in str(e):\n",
    "            different_neighbor.add(e[0])\n",
    "            different_neighbor.add(e[1])\n",
    "    return len(different_neighbor)-2\n",
    "\n",
    "def cal_anomaly_loss(loss_list,edge_list,file_path):\n",
    "    \n",
    "    if len(loss_list)!=len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count=0\n",
    "    loss_sum=0\n",
    "    loss_std=std(loss_list)\n",
    "    loss_mean=mean(loss_list)\n",
    "    edge_set=set()\n",
    "    node_set=set()\n",
    "    node2redundant={}\n",
    "    \n",
    "    thr=loss_mean+1.5*loss_std\n",
    "\n",
    "    print(\"thr:\",thr)\n",
    "  \n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i]>thr:\n",
    "            count+=1\n",
    "            src_node=edge_list[i][0]\n",
    "            dst_node=edge_list[i][1]\n",
    "          \n",
    "            loss_sum+=loss_list[i]\n",
    "    \n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0]+edge_list[i][1])\n",
    "    return count, loss_sum/(count+0.00000000001),node_set,edge_set\n",
    "#     return count, count/len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_IDF={}\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_3/\"\n",
    "file_l=os.listdir(\"graph_4_3/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "    \n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']) or True:\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']) or True:\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF \n",
    "\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF_4_3-5\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF={}\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_10/\"\n",
    "file_l=os.listdir(\"graph_4_10/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']) or True:\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']) or True:\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF \n",
    "    \n",
    "\n",
    "torch.save(node_IDF,\"node_IDF_4_10\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF={}\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']) or True:\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']) or True:\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF \n",
    "\n",
    "\n",
    "torch.save(node_IDF,\"node_IDF_4_11\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_IDF={}\n",
    "node_set=set()\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_12/\"\n",
    "file_l=os.listdir(\"graph_4_12/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "\n",
    "\n",
    "node_set = {}\n",
    "for f_path in tqdm(file_list):\n",
    "    f=open(f_path)\n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        if jdata['loss']>0:\n",
    "            if 'netflow' not in str(jdata['srcmsg']) or True:\n",
    "                if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['srcmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "            if 'netflow' not in str(jdata['dstmsg']) or True:\n",
    "                if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                    node_set[str(jdata['dstmsg'])] = set([f_path])\n",
    "                else:\n",
    "                    node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "for n in node_set:\n",
    "    include_count = len(node_set[n])   \n",
    "    IDF=math.log(len(file_list)/(include_count+1))\n",
    "    node_IDF[n] = IDF \n",
    "    \n",
    "\n",
    "torch.save(node_IDF,\"node_IDF_4_12\")\n",
    "print(\"IDF weight calculate complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the relations between time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-10,11\n",
    "def is_include_key_word_bak(s):\n",
    "    keywords=[\n",
    "         'netflow',\n",
    "        'null',\n",
    "        '/dev/pts',\n",
    "        'salt-minion.log',\n",
    "        '675',\n",
    "        'usr',\n",
    "         'proc',\n",
    "        '/.cache/mozilla/',\n",
    "        'tmp',\n",
    "        'thunderbird',\n",
    "        '/bin/',\n",
    "        '/sbin/sysctl',\n",
    "        '/data/replay_logdb/',\n",
    "        '/home/admin/eraseme',\n",
    "        \n",
    "        '/stat',\n",
    "        \n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "def cal_set_rel_bak(s1,s2,file_list):\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "        if is_include_key_word_bak(i) is not True:\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))         \n",
    "\n",
    "            if (IDF)>math.log(len(file_list)*0.9/(1))  :\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-12\n",
    "\n",
    "# def is_include_key_word_bak(s):\n",
    "#     keywords=[\n",
    "#          'netflow',        \n",
    "#         '/dev/pts',\n",
    "#         'salt-minion.log',\n",
    "#         'null',\n",
    "#         'usr',\n",
    "#          'proc',\n",
    "#         'firefox',\n",
    "#         'tmp',\n",
    "#         'thunderbird',\n",
    "#         'bin/',\n",
    "#         '/data/replay_logdb',\n",
    "#         '/stat',\n",
    "#         '/boot',\n",
    "#         'qt-opensource-linux-x64',\n",
    "#         '/eraseme',\n",
    "#         '675',\n",
    "        \n",
    "# #       \n",
    "# #         'etc',  \n",
    "# #         'cdrom', \n",
    "# #         'shm'\n",
    "#       ]\n",
    "#     flag=False\n",
    "#     for i in keywords:\n",
    "#         if i in s:\n",
    "#             flag=True\n",
    "#     return flag\n",
    "\n",
    "# def cal_set_rel_bak(s1,s2,file_list):\n",
    "#     new_s=s1 & s2\n",
    "#     count=0\n",
    "#     for i in new_s:\n",
    "# #     jdata=json.loads(i)\n",
    "#         if is_include_key_word_bak(i) is not True:\n",
    "#             if i in node_IDF.keys():\n",
    "#                 IDF=node_IDF[i]\n",
    "#             else:\n",
    "#                 IDF=math.log(len(file_list)/(1))         \n",
    "\n",
    "#             if (IDF)>math.log(len(file_list)*0.9/(1))  :\n",
    "#                 print(\"node:\",i,\" IDF:\",IDF)\n",
    "#                 count+=1\n",
    "#     return count\n",
    "\n",
    "\n",
    "def is_include_key_word(s):\n",
    "    keywords=[\n",
    "         'netflow',        \n",
    "        '/dev/pts',\n",
    "        'salt-minion.log',\n",
    "        'null',\n",
    "        'usr',\n",
    "         'proc',\n",
    "        'firefox',\n",
    "        'tmp',\n",
    "        'thunderbird',\n",
    "        'bin/',\n",
    "        '/data/replay_logdb',\n",
    "        '/stat',\n",
    "        '/boot',\n",
    "        'qt-opensource-linux-x64',\n",
    "        '/eraseme',\n",
    "        '675',\n",
    "      ]\n",
    "    flag=False\n",
    "    for i in keywords:\n",
    "        if i in s:\n",
    "            flag=True\n",
    "    return flag\n",
    "\n",
    "\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_12/\"\n",
    "file_l=os.listdir(\"graph_4_12/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "\n",
    "\n",
    "def cal_set_rel(s1,s2,file_list, file_list_4_3_5):\n",
    "    IDF3 = node_IDF_3\n",
    "    new_s=s1 & s2\n",
    "    count=0\n",
    "    for i in new_s:\n",
    "#     jdata=json.loads(i)\n",
    "       if is_include_key_word(i) is not True:\n",
    "        \n",
    "#         'netflow' not in i\n",
    "#         and 'usr' not in i and 'var' not in i\n",
    "            if i in node_IDF.keys():\n",
    "                IDF=node_IDF[i]\n",
    "            else:\n",
    "                IDF=math.log(len(file_list)/(1))\n",
    "                \n",
    "            if i in node_IDF_3.keys():\n",
    "                IDF3=node_IDF_3[i]\n",
    "            else:\n",
    "                IDF3=math.log(len(file_list_4_3_5)/(1))    \n",
    "            \n",
    "#             print(IDF)\n",
    "            if (IDF+IDF3)>5 :\n",
    "                print(\"node:\",i,\" IDF:\",IDF)\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_10/\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_10/\"+f]=0\n",
    "\n",
    "filelist = os.listdir(\"graph_4_11/\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_11/\"+f]=0\n",
    "    \n",
    "filelist = os.listdir(\"graph_4_12/\")\n",
    "for f in filelist:\n",
    "    labels[\"graph_4_12/\"+f]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list=[\n",
    "    \n",
    "'graph_4_10/2018-04-10 13:31:14.548738409~2018-04-10 13:46:36.161065223.txt',\n",
    "'graph_4_10/2018-04-10 14:02:17.001271389~2018-04-10 14:17:34.001373488.txt',\n",
    "'graph_4_10/2018-04-10 14:17:34.001373488~2018-04-10 14:33:18.350772859.txt',\n",
    "'graph_4_10/2018-04-10 14:33:18.350772859~2018-04-10 14:48:47.320442910.txt',\n",
    "'graph_4_10/2018-04-10 14:48:47.320442910~2018-04-10 15:03:54.307022037.txt', \n",
    " \n",
    "'graph_4_12/2018-04-12 12:39:06.592684498~2018-04-12 12:54:44.001888457.txt',\n",
    "'graph_4_12/2018-04-12 12:54:44.001888457~2018-04-12 13:09:55.026832462.txt',\n",
    "'graph_4_12/2018-04-12 13:09:55.026832462~2018-04-12 13:25:06.588370709.txt',\n",
    "'graph_4_12/2018-04-12 13:25:06.588370709~2018-04-12 13:40:07.178206094.txt',\n",
    "]\n",
    "\n",
    "for i in attack_list:\n",
    "    labels[i]=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node_IDF_3=torch.load(\"node_IDF_4_3\")\n",
    "node_IDF=torch.load(\"node_IDF_4_3-5\")\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_9/\"\n",
    "file_l=os.listdir(\"graph_4_9/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "    \n",
    "    \n",
    "# node_IDF_410=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF=torch.load(\"node_IDF_4_12\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_9/\"\n",
    "file_l=os.listdir(\"graph_4_9/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "# file_path=\"graph_4_12/\"\n",
    "# file_l=os.listdir(\"graph_4_12/\")\n",
    "# for i in file_l:\n",
    "#     file_path_list.append(file_path+i)\n",
    "\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10_without_neg_edge/\")\n",
    "    current_tw={}\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "        if added_que_flag:\n",
    "            break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pred_label={}\n",
    "\n",
    "# files = os.listdir(\"graph_4_9\")\n",
    "# for f in files:\n",
    "#     pred_label[\"graph_4_9/\"+f]=0\n",
    "\n",
    "# files = os.listdir(\"graph_4_10\")\n",
    "# for f in files:\n",
    "#     pred_label[\"graph_4_10/\"+f]=0\n",
    "\n",
    "# files = os.listdir(\"graph_4_11\")\n",
    "# for f in files:\n",
    "#     pred_label[\"graph_4_11/\"+f]=0\n",
    "\n",
    "# files = os.listdir(\"graph_4_12\")\n",
    "# for f in files:\n",
    "#     pred_label[\"graph_4_12/\"+f]=0\n",
    "\n",
    "\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "\n",
    "    if loss_count>10:\n",
    "#     if loss_count>50:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "            print(i['name'])\n",
    "#         print(name_list)\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_IDF_3=torch.load(\"node_IDF_4_3\")\n",
    "node_IDF=torch.load(\"node_IDF_4_3-5\")\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_10/\"\n",
    "file_l=os.listdir(\"graph_4_10/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "    \n",
    "    \n",
    "# node_IDF_410=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF=torch.load(\"node_IDF_4_12\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_10/\"\n",
    "file_l=os.listdir(\"graph_4_10/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "# file_path=\"graph_4_12/\"\n",
    "# file_l=os.listdir(\"graph_4_12/\")\n",
    "# for i in file_l:\n",
    "#     file_path_list.append(file_path+i)\n",
    "\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_10_without_neg_edge/\")\n",
    "    current_tw={}\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "        if added_que_flag:\n",
    "            break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_label={}\n",
    "\n",
    "# files = os.listdir(\"graph_4_9\")\n",
    "# for f in files:\n",
    "#     pred_label[\"graph_4_9/\"+f]=0\n",
    "\n",
    "files = os.listdir(\"graph_4_10\")\n",
    "for f in files:\n",
    "    pred_label[\"graph_4_10/\"+f]=0\n",
    "\n",
    "files = os.listdir(\"graph_4_11\")\n",
    "for f in files:\n",
    "    pred_label[\"graph_4_11/\"+f]=0\n",
    "\n",
    "files = os.listdir(\"graph_4_12\")\n",
    "for f in files:\n",
    "    pred_label[\"graph_4_12/\"+f]=0\n",
    "\n",
    "\n",
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "\n",
    "    if loss_count>14:\n",
    "#     if loss_count>50:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "            print(i['name'])\n",
    "#         print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node_IDF_3=torch.load(\"node_IDF_4_3\")\n",
    "node_IDF=torch.load(\"node_IDF_4_3-5\")\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "    \n",
    "    \n",
    "# node_IDF_410=torch.load(\"node_IDF_4_10\")\n",
    "# node_IDF=torch.load(\"node_IDF_4_12\")\n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "# node_set_list=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_11/\"\n",
    "file_l=os.listdir(\"graph_4_11/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "# file_path=\"graph_4_12/\"\n",
    "# file_l=os.listdir(\"graph_4_12/\")\n",
    "# for i in file_l:\n",
    "#     file_path_list.append(file_path+i)\n",
    "\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_11_without_neg_edge/\")\n",
    "    current_tw={}\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_list)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                print(\"history queue:\",his_tw['name'])\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "        if added_que_flag:\n",
    "            break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "  \n",
    "    index_count+=1\n",
    "#     node_set_list.append(node_set)\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "#     y_data_4_10.append([loss_avg,labels_4_10[f_path],f_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>25:\n",
    "#     if loss_count>50:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(name_list)\n",
    "#         for i in name_list:\n",
    "#             pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_3_5=[]\n",
    "\n",
    "file_path=\"graph_4_3/\"\n",
    "file_l=os.listdir(\"graph_4_3/\")\n",
    "for i in file_l:\n",
    "    file_list_3_5.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_4/\"\n",
    "file_l=os.listdir(\"graph_4_4/\")\n",
    "for i in file_l:\n",
    "    file_list_3_5.append(file_path+i)\n",
    "\n",
    "file_path=\"graph_4_5/\"\n",
    "file_l=os.listdir(\"graph_4_5/\")\n",
    "for i in file_l:\n",
    "    file_list_3_5.append(file_path+i)\n",
    "    \n",
    "len(file_list_3_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_IDF=torch.load(\"node_IDF_4_12\") \n",
    "node_IDF_3=torch.load(\"node_IDF_4_3-5\")\n",
    "# node_IDF=torch.load(\"node_IDF_4_3\")\n",
    "file_list=[]\n",
    "\n",
    "file_path=\"graph_4_12/\"\n",
    "file_l=os.listdir(\"graph_4_12/\")\n",
    "for i in file_l:\n",
    "    file_list.append(file_path+i)\n",
    "    \n",
    "# the variable names doesn't change the results.   \n",
    "y_data_4_10=[]\n",
    "df_list_4_10=[]\n",
    "history_list=[]\n",
    "tw_que=[]\n",
    "his_tw={}\n",
    "current_tw={}\n",
    "\n",
    "\n",
    "\n",
    "file_path_list=[]\n",
    "\n",
    "\n",
    "file_path=\"graph_4_12/\"\n",
    "file_l=os.listdir(\"graph_4_12/\")\n",
    "for i in file_l:\n",
    "    file_path_list.append(file_path+i)\n",
    "\n",
    "\n",
    "index_count=0\n",
    "for f_path in sorted(file_path_list):\n",
    "    f=open(f_path)\n",
    "    edge_loss_list=[]\n",
    "    edge_list=[]\n",
    "    print('index_count:',index_count)\n",
    "    \n",
    "    for line in f:\n",
    "        l=line.strip()\n",
    "        jdata=eval(l)\n",
    "        edge_loss_list.append(jdata['loss'])\n",
    "        edge_list.append([str(jdata['srcmsg']),str(jdata['dstmsg'])])\n",
    "    df_list_4_10.append(pd.DataFrame(edge_loss_list))\n",
    "    count,loss_avg,node_set,edge_set=cal_anomaly_loss(edge_loss_list,edge_list,\"graph_4_12_without_neg_edge/\")\n",
    "    current_tw={}\n",
    "    current_tw['name']=f_path\n",
    "    current_tw['loss']=loss_avg\n",
    "    current_tw['index']=index_count\n",
    "    current_tw['nodeset']=node_set\n",
    "\n",
    "    added_que_flag=False\n",
    "    for hq in history_list:\n",
    "        for his_tw in hq:\n",
    "            cal_re = cal_set_rel(current_tw['nodeset'],his_tw['nodeset'],file_list, file_list_3_5)\n",
    "            if cal_re != 0 and current_tw['name']!=his_tw['name']:\n",
    "#             if cal_set_rel_bak(current_tw['nodeset'],his_tw['nodeset'],file_l)!=0 and current_tw['name']!=his_tw['name']:\n",
    "                hq.append(copy.deepcopy(current_tw))\n",
    "                added_que_flag=True\n",
    "                break\n",
    "        if added_que_flag:\n",
    "            break\n",
    "    if added_que_flag is False:\n",
    "        temp_hq=[copy.deepcopy(current_tw)]\n",
    "        history_list.append(temp_hq)\n",
    "    index_count+=1\n",
    "    print( f_path,\"  \",loss_avg,\" count:\",count,\" percentage:\",count/len(edge_list),\" node count:\",len(node_set),\" edge count:\",len(edge_set))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for i in history_list:\n",
    "#         print(len(i))\n",
    "#         if len(i) >= 2:\n",
    "#             for tw in i:\n",
    "#                 print(tw['name'])\n",
    "#     input()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hl in history_list:\n",
    "    loss_count=0\n",
    "    for hq in hl:\n",
    "        if loss_count==0:\n",
    "            loss_count=(loss_count+1)*(hq['loss']+1)\n",
    "        else:\n",
    "            loss_count=(loss_count)*(hq['loss']+1)\n",
    "    name_list=[]\n",
    "    if loss_count>20:\n",
    "#     if loss_count>50:\n",
    "        name_list=[]\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        print(name_list)\n",
    "        for i in name_list:\n",
    "            pred_label[i]=1\n",
    "        print(loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_thr():\n",
    "    np.seterr(invalid='ignore')\n",
    "    step=0.01\n",
    "    thr_list=torch.arange(-5,5,step)\n",
    "    \n",
    "    \n",
    "\n",
    "    precision_list=[]\n",
    "    recall_list=[]\n",
    "    fscore_list=[]\n",
    "    accuracy_list=[]\n",
    "    auc_val_list=[]\n",
    "    for thr in thr_list:\n",
    "        threshold=thr\n",
    "        y_prediction=[]\n",
    "        for i in y_test_scores:\n",
    "            if i >threshold:\n",
    "                y_prediction.append(1)\n",
    "            else:\n",
    "                y_prediction.append(0)\n",
    "        precision,recall,fscore,accuracy,auc_val=classifier_evaluation(y_test, y_prediction)   \n",
    "        precision_list.append(float(precision))\n",
    "        recall_list.append(float(recall))\n",
    "        fscore_list.append(float(fscore))\n",
    "        accuracy_list.append(float(accuracy))\n",
    "        auc_val_list.append(float(auc_val))\n",
    "\n",
    "    max_fscore=max(fscore_list)\n",
    "    max_fscore_index=fscore_list.index(max_fscore)\n",
    "    print(max_fscore_index)\n",
    "    print(\"max threshold:\",thr_list[max_fscore_index])\n",
    "    print('precision:',precision_list[max_fscore_index])\n",
    "    print('recall:',recall_list[max_fscore_index])\n",
    "    print('fscore:',fscore_list[max_fscore_index])\n",
    "    print('accuracy:',accuracy_list[max_fscore_index])    \n",
    "    print('auc:',auc_val_list[max_fscore_index])\n",
    "    \n",
    "         \n",
    "#     precision_list=torch.tensor(precision_list)   \n",
    "#     recall_list=torch.tensor(recall_list)   \n",
    "#     fscore_list=torch.tensor(fscore_list)   \n",
    "#     accuracy_list=torch.tensor(accuracy_list)   \n",
    "#     auc_val_list=torch.tensor(auc_val_list)   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # plt.scatter(attack_x, attack_y, s=20, c='r', label='Attack graph',marker='*')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "    # plt.scatter(bengin_x, bengin_y, s=20, c='g', label='Bengin graph',marker='1')\n",
    "\n",
    "    plt.plot(thr_list,precision_list,color='red',label='precision',linewidth=2.0,linestyle='-')\n",
    "    plt.plot(thr_list,recall_list,color='orange',label='recall',linewidth=2.0,linestyle='solid')\n",
    "    plt.plot(thr_list,fscore_list,color='y',label='F-score',linewidth=2.0,linestyle='dashed')\n",
    "    plt.plot(thr_list,accuracy_list,color='g',label='accuracy',linewidth=2.0,linestyle='dashdot')\n",
    "    plt.plot(thr_list,auc_val_list,color='b',label='auc_val',linewidth=2.0,linestyle='dotted')\n",
    "    # '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "\n",
    "\n",
    "    # plt.scatter(turnovers, graph_loss, c=color)\n",
    "    plt.xlabel(\"Threshold\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Rate\", fontdict={'size': 16})\n",
    "    plt.title(\"Different evaluation Indicators by varying threshold value\", fontdict={'size': 12})\n",
    "    plt.legend(loc='best', fontsize=12, markerscale=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def classifier_evaluation(y_test, y_test_pred):\n",
    "    # groundtruth, pred_value\n",
    "    tn, fp, fn, tp =confusion_matrix(y_test, y_test_pred).ravel()\n",
    "#     tn+=100\n",
    "#     print(clf_name,\" : \")\n",
    "    print('tn:',tn)\n",
    "    print('fp:',fp)\n",
    "    print('fn:',fn)\n",
    "    print('tp:',tp)\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    fscore=2*(precision*recall)/(precision+recall)    \n",
    "    auc_val=roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"fscore:\",fscore)\n",
    "    print(\"accuracy:\",accuracy)\n",
    "    print(\"auc_val:\",auc_val)\n",
    "    return precision,recall,fscore,accuracy,auc_val\n",
    "\n",
    "def minmax(data):\n",
    "    min_val=min(data)\n",
    "    max_val=max(data)\n",
    "    ans=[]\n",
    "    for i in data:\n",
    "        ans.append((i-min_val)/(max_val-min_val))\n",
    "    return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "y_pred=[]\n",
    "for i in labels:\n",
    "    y.append(labels[i])\n",
    "    y_pred.append(pred_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_evaluation(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the attack edges numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_hit(line):\n",
    "    attack_nodes=[\n",
    "            '/home/admin/clean',\n",
    "            '/dev/glx_alsa_675',\n",
    "            '/home/admin/profile',\n",
    "#             '/var/log/mail',  \n",
    "            '/tmp/memtrace.so',\n",
    "            '/var/log/xdev',\n",
    "             '/var/log/wdev',\n",
    "            'gtcache',\n",
    "#             'firefox',\n",
    "        '161.116.88.72',\n",
    "        '146.153.68.151',\n",
    "        '145.199.103.57',\n",
    "        '61.130.69.232',\n",
    "        '5.214.163.155',\n",
    "        '104.228.117.212',\n",
    "        '141.43.176.203',\n",
    "        '7.149.198.40',\n",
    "        '5.214.163.155',\n",
    "        '149.52.198.23',\n",
    "        ]\n",
    "    flag=False\n",
    "    for i in attack_nodes:\n",
    "        if i in line:\n",
    "            flag=True\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "files=[]\n",
    "temp_file=[\n",
    "        '2018-04-10 13:31:14.548738409~2018-04-10 13:46:36.161065223.txt',\n",
    "        '2018-04-10 14:02:17.001271389~2018-04-10 14:17:34.001373488.txt',\n",
    "        '2018-04-10 14:17:34.001373488~2018-04-10 14:33:18.350772859.txt',\n",
    "        '2018-04-10 14:33:18.350772859~2018-04-10 14:48:47.320442910.txt',\n",
    "        '2018-04-10 14:48:47.320442910~2018-04-10 15:03:54.307022037.txt',\n",
    "]\n",
    "for f in temp_file:\n",
    "    files.append(\"./graph_4_10/\"+f)\n",
    "    \n",
    "    \n",
    "temp_file=[\n",
    "         '2018-04-12 12:39:06.592684498~2018-04-12 12:54:44.001888457.txt',\n",
    "        '2018-04-12 12:54:44.001888457~2018-04-12 13:09:55.026832462.txt',\n",
    "        '2018-04-12 13:09:55.026832462~2018-04-12 13:25:06.588370709.txt',\n",
    "        '2018-04-12 13:25:06.588370709~2018-04-12 13:40:07.178206094.txt',\n",
    "]    \n",
    "for f in temp_file:\n",
    "    files.append(\"./graph_4_12/\"+f)\n",
    "    \n",
    "attack_edge_count=0\n",
    "for fpath in tqdm(files):\n",
    "    f=open(fpath)\n",
    "    for line in f:\n",
    "        if keyword_hit(line):\n",
    "            attack_edge_count+=1\n",
    "print(attack_edge_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from graphviz import Digraph\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import community.community_louvain as community_louvain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Some common path abstraction for visualization\n",
    "replace_dic = {\n",
    "        '/run/shm/':'/run/shm/*',\n",
    "        #     '/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/':'/home/admin/.cache/mozilla/firefox/pe11scpa.default/cache2/entries/*',\n",
    "        '/home/admin/.cache/mozilla/firefox/':'/home/admin/.cache/mozilla/firefox/*',\n",
    "        '/home/admin/.mozilla/firefox':'/home/admin/.mozilla/firefox*',\n",
    "        '/data/replay_logdb/':'/data/replay_logdb/*',\n",
    "        '/home/admin/.local/share/applications/':'/home/admin/.local/share/applications/*',\n",
    "        '/usr/share/applications/':'/usr/share/applications/*',\n",
    "        '/lib/x86_64-linux-gnu/':'/lib/x86_64-linux-gnu/*',\n",
    "        '/proc/':'/proc/*',\n",
    "        '/stat':'*/stat',\n",
    "        '/etc/bash_completion.d/':'/etc/bash_completion.d/*',\n",
    "        '/usr/bin/python2.7':'/usr/bin/python2.7/*',\n",
    "        '/usr/lib/python2.7':'/usr/lib/python2.7/*',\n",
    "}\n",
    "\n",
    "\n",
    "def replace_path_name(path_name):\n",
    "    for i in replace_dic:\n",
    "        if i in path_name:\n",
    "            return replace_dic[i]\n",
    "    return path_name\n",
    "\n",
    "\n",
    "# Users should manually put the detected anomalous time windows here\n",
    "attack_list = [\n",
    "        'graph_4_10/2018-04-10 13:31:14.548738409~2018-04-10 13:46:36.161065223.txt',\n",
    "        'graph_4_10/2018-04-10 14:02:17.001271389~2018-04-10 14:17:34.001373488.txt',\n",
    "        'graph_4_10/2018-04-10 14:17:34.001373488~2018-04-10 14:33:18.350772859.txt',\n",
    "        'graph_4_10/2018-04-10 14:33:18.350772859~2018-04-10 14:48:47.320442910.txt',\n",
    "        'graph_4_10/2018-04-10 14:48:47.320442910~2018-04-10 15:03:54.307022037.txt',\n",
    "    \n",
    "        'graph_4_12/2018-04-12 12:39:06.592684498~2018-04-12 12:54:44.001888457.txt', \n",
    "        'graph_4_12/2018-04-12 12:54:44.001888457~2018-04-12 13:09:55.026832462.txt', \n",
    "        'graph_4_12/2018-04-12 13:09:55.026832462~2018-04-12 13:25:06.588370709.txt', \n",
    "        'graph_4_12/2018-04-12 13:25:06.588370709~2018-04-12 13:40:07.178206094.txt'\n",
    "]\n",
    "\n",
    "original_edges_count = 0\n",
    "graphs = []\n",
    "gg = nx.DiGraph()\n",
    "count = 0\n",
    "for path in tqdm(attack_list):\n",
    "    if \".txt\" in path:\n",
    "        line_count = 0\n",
    "        node_set = set()\n",
    "        tempg = nx.DiGraph()\n",
    "        f = open(path, \"r\")\n",
    "        edge_list = []\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            l = line.strip()\n",
    "            jdata = eval(l)\n",
    "            edge_list.append(jdata)\n",
    "\n",
    "        edge_list = sorted(edge_list, key=lambda x: x['loss'], reverse=True)\n",
    "        original_edges_count += len(edge_list)\n",
    "\n",
    "        loss_list = []\n",
    "        for i in edge_list:\n",
    "            loss_list.append(i['loss'])\n",
    "        loss_mean = mean(loss_list)\n",
    "        loss_std = std(loss_list)\n",
    "        print(loss_mean)\n",
    "        print(loss_std)\n",
    "        thr = loss_mean + 1.5 * loss_std\n",
    "        print(\"thr:\", thr)\n",
    "        for e in edge_list:\n",
    "            if e['loss'] > thr:\n",
    "                tempg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))),\n",
    "                               str(hashgen(replace_path_name(e['dstmsg']))))\n",
    "                gg.add_edge(str(hashgen(replace_path_name(e['srcmsg']))), str(hashgen(replace_path_name(e['dstmsg']))),\n",
    "                            loss=e['loss'], srcmsg=e['srcmsg'], dstmsg=e['dstmsg'], edge_type=e['edge_type'],\n",
    "                            time=e['time'])\n",
    "\n",
    "\n",
    "partition = community_louvain.best_partition(gg.to_undirected())\n",
    "\n",
    "# Generate the candidate subgraphs based on community discovery results\n",
    "communities = {}\n",
    "max_partition = 0\n",
    "for i in partition:\n",
    "    if partition[i] > max_partition:\n",
    "        max_partition = partition[i]\n",
    "for i in range(max_partition + 1):\n",
    "    communities[i] = nx.DiGraph()\n",
    "for e in gg.edges:\n",
    "    communities[partition[e[0]]].add_edge(e[0], e[1])\n",
    "    communities[partition[e[1]]].add_edge(e[0], e[1])\n",
    "\n",
    "\n",
    "# Define the attack nodes. They are **only be used to plot the colors of attack nodes and edges**.\n",
    "# They won't change the detection results.\n",
    "def attack_edge_flag(msg):\n",
    "    attack_nodes = [\n",
    "        '/home/admin/clean',\n",
    "        '/dev/glx_alsa_675',\n",
    "        '/home/admin/profile',\n",
    "        '/var/log/xdev',\n",
    "        '/etc/passwd',\n",
    "        '161.116.88.72',\n",
    "        '146.153.68.151',\n",
    "        '/var/log/mail',\n",
    "        '/tmp/memtrace.so',\n",
    "        #         '/tmp',\n",
    "        '/var/log/xdev',\n",
    "        '/var/log/wdev',\n",
    "        'gtcache',\n",
    "        'firefox',\n",
    "        #         '/var/log',\n",
    "    ]\n",
    "    flag = False\n",
    "    for i in attack_nodes:\n",
    "        if i in str(msg):\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "# Plot and render candidate subgraph\n",
    "os.system(f\"mkdir -p ./graph_visual/\")\n",
    "graph_index = 0\n",
    "for c in communities:\n",
    "    dot = Digraph(name=\"MyPicture\", comment=\"the test\", format=\"pdf\")\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    graphml_graph = nx.DiGraph()\n",
    "\n",
    "\n",
    "    for e in communities[c].edges:\n",
    "        try:\n",
    "            temp_edge = gg.edges[e]\n",
    "            srcnode = e['srcnode']\n",
    "            dstnode = e['dstnode']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if True:\n",
    "            # source node\n",
    "            if \"'subject': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['srcmsg']:\n",
    "                src_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['srcmsg']):\n",
    "                src_node_color = 'red'\n",
    "            else:\n",
    "                src_node_color = 'blue'\n",
    "                \n",
    "            src_name= str(hashgen(replace_path_name(temp_edge['srcmsg'])))\n",
    "            dot.node(name=src_name, label=str(\n",
    "                replace_path_name(temp_edge['srcmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['srcmsg'])))])), color=src_node_color,\n",
    "                     shape=src_shape)\n",
    "\n",
    "            # destination node\n",
    "            if \"'subject': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'box'\n",
    "            elif \"'file': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'oval'\n",
    "            elif \"'netflow': '\" in temp_edge['dstmsg']:\n",
    "                dst_shape = 'diamond'\n",
    "            if attack_edge_flag(temp_edge['dstmsg']):\n",
    "                dst_node_color = 'red'\n",
    "            else:\n",
    "                dst_node_color = 'blue'\n",
    "\n",
    "            dst_name=str(hashgen(replace_path_name(temp_edge['dstmsg'])))\n",
    "            dot.node(name=dst_name, label=str(\n",
    "                replace_path_name(temp_edge['dstmsg']) + str(\n",
    "                    partition[str(hashgen(replace_path_name(temp_edge['dstmsg'])))])), color=dst_node_color,\n",
    "                     shape=dst_shape)\n",
    "\n",
    "            if attack_edge_flag(temp_edge['srcmsg']) and attack_edge_flag(temp_edge['dstmsg']):\n",
    "                edge_color = 'red'\n",
    "            else:\n",
    "                edge_color = 'blue'\n",
    "            dot.edge(str(hashgen(replace_path_name(temp_edge['srcmsg']))),\n",
    "                     str(hashgen(replace_path_name(temp_edge['dstmsg']))), label=temp_edge['edge_type'],\n",
    "                     color=edge_color)\n",
    "            \n",
    "             # Add nodes and edge to the NetworkX graph\n",
    "            graphml_graph.add_node(src_name, label=replace_path_name(temp_edge['srcmsg']), color=src_node_color, shape=src_shape)\n",
    "            graphml_graph.add_node(dst_name, label=replace_path_name(temp_edge['dstmsg']), color=dst_node_color, shape=dst_shape)\n",
    "            graphml_graph.add_edge(src_name, dst_name, label=temp_edge['edge_type'], color=edge_color)\n",
    "\n",
    "    dot.render(f'./graph_visual/subgraph_' + str(graph_index), view=False)\n",
    "    nx.write_graphml(graphml_graph, f'./graph_visual/subgraph_' + str(graph_index) + '.graphml')\n",
    "    graph_index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos kernel",
   "language": "python",
   "name": "kairos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
